{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, load_metric,load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We must tokenise the input before feeding to the model. We use the tokenizer from the pretrained model.\n",
    "- The dataset must have the label column named as 'label', to be trained using Trainer API. It is preferable to keep the text column as 'text'. However since we are tokenising the text column, it can be named anything, as we just pass the tokenised columns input_ids and attention_mask to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_name = \"roberta-base-emotion-prediction-phr-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"anger\":0,\n",
    "    \"anticipation\":1,\n",
    "    \"disgust\":1,\n",
    "    \"fear\":1,\n",
    "    \"joy\":1,\n",
    "    \"love\":1,\n",
    "    \"optimism\":1,\n",
    "    \"pessimism\":1,\n",
    "    \"sadness\":1,\n",
    "    \"surprise\":1,\n",
    "    \"trust\":1\n",
    "}\n",
    "\n",
    "id2label = {\n",
    "    0:\"anger\",\n",
    "    1:\"anticipation\",\n",
    "    2:\"disgust\",\n",
    "    3:\"fear\",\n",
    "    4:\"joy\",\n",
    "    5:\"love\",\n",
    "    6:\"optimism\",\n",
    "    7:\"pessimism\",\n",
    "    8:\"sadness\",\n",
    "    9:\"surprise\",\n",
    "    10:\"trust\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\",problem_type =\"multi_label_classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", problem_type =\"multi_label_classification\", num_labels=11,label2id=label2id,id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vibhorag101/sem_eval_2018_task_1_english_cleaned_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniseDataset(dataset):\n",
    "    return(tokeniser(dataset[\"text\"],padding=\"max_length\",truncation=True))\n",
    "\n",
    "#\n",
    "dataset.set_format(\"torch\")\n",
    "dataset = dataset.map(lambda x : {\"float_labels\": x[\"labels\"].to(torch.float)}, remove_columns=[\"labels\"]).rename_column(\"float_labels\", \"labels\")\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "column_names.remove(\"labels\")\n",
    "tokenisedDataset = dataset.map(tokeniseDataset,batched=True,remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTokeniseDataset = tokenisedDataset[\"train\"]\n",
    "testTokenisedDataset = tokenisedDataset[\"test\"]\n",
    "valTokenisedDataset = tokenisedDataset[\"test\"]\n",
    "print(trainTokeniseDataset)\n",
    "print(valTokenisedDataset)\n",
    "print(testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score,precision_score,recall_score\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    sigmoid_pred = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into predicted labels\n",
    "    y_pred  = np.where(sigmoid_pred > threshold, 1, 0)\n",
    "\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    precision = precision_score(y_true, y_pred, average='micro')\n",
    "    recall = recall_score(y_true, y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    metrics = { 'accuracy': accuracy,\n",
    "                'micro_precision': precision,\n",
    "                'micro_recall': recall,\n",
    "                'micro_f1': f1_micro_average,\n",
    "                'micro_roc_auc': roc_auc,\n",
    "                }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return multi_label_metrics(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"huggingface\", entity=\"vibhor20349\", name=finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=finetune_model_name,\n",
    "    report_to = 'wandb',\n",
    "    learning_rate=2e-5, # recommended in roberta paper\n",
    "    num_train_epochs=10, #recommended in bert paper\n",
    "    per_device_train_batch_size=16, # recommended in roberta paper\n",
    "    per_device_eval_batch_size=16, # recommended in roberta paper\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_steps=100,\n",
    "    push_to_hub=True #to clone the training repo just before starting to train, so push to hub works\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainTokeniseDataset,\n",
    "    eval_dataset=valTokenisedDataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokeniser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer.predict(testTokenisedDataset))\n",
    "trainer.evaluate(eval_dataset=testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IF we want to make predictions\n",
    "input_text = \"You are an extremely good person. Thank you so much\"\n",
    "inputs = tokeniser(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    ## BERT expects the keyword agruments \"token_type_ids\" and \"attention_mask\" in input.\n",
    "    # so we convert inputs dictionary to keyword arguments using ** before passing to the model\n",
    "    inputs.to('cuda')\n",
    "    outputs = model(**inputs)\n",
    "    ## output of Bert contains logits for each class, pooled output and hidden states and attentions\n",
    "\n",
    "# Extract the predicted logits(raw values for each class)\n",
    "logits = outputs.logits\n",
    "print(outputs)\n",
    "\n",
    "## logits are just raw values for each class. To get probabilities we use softmax\n",
    "sigmoid_logits = torch.nn.functional.sigmoid(logits)\n",
    "predictions = np.where(sigmoid_logits.cpu() > 0.5, 1, 0)\n",
    "\n",
    "# Get the predicted labels from id2label dictionary\n",
    "label_predictions = []\n",
    "for pred in predictions:\n",
    "    label_predictions.append([id2label[i] for i, val in enumerate(pred) if val])\n",
    "print(label_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
